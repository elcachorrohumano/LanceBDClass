{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejercicio descargamos una base de datos de mensajes de texto en inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"chirunder/text_messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset['train'])\n",
    "df.rename(columns={'text': 'texto'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para los siguientes ejercicios, voy a crear una variable del numero de palabras en cada mensaje de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n'] = df['texto'].apply(lambda x: len(str(x).split()))\n",
    "df = df[['n', 'texto']]\n",
    "df.head(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1:\n",
    "A partir del dataframe df, crea df_tokenized (usando el Tokenizer de GPT2) con dos columnas pero con el texto tokenizado. Asegurate de que todos los embeddings tengan la misma longitud y los tokens sean enteros (todos enteros o todos doubles). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "\n",
    "tokenized_text = df['texto'].apply(lambda x: tokenizer(x)[\"input_ids\"])\n",
    "\n",
    "tokenized_text = tokenized_text.apply(lambda x: x[:300] + [0] * (300 - len(x)) if len(x) < 300 else x[:300])\n",
    "\n",
    "\n",
    "df_tokenized = pd.DataFrame({'vector': tokenized_text, 'name': df['n']})\n",
    "\n",
    "df_tokenized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2:\n",
    "Mete el dataframe a una tabla en una base de datos de LanceDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resuelve el task 2 aqui\n",
    "db = lancedb.connect(\"./.lancedb\")\n",
    "# Creamos una tabla en la base de datos\n",
    "db.create_table(\"tabla\", df_tokenized)\n",
    "db[\"tabla\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3:\n",
    "Haz una query estilo SQL a la tabla de la base de datos. Quiero que escribas la query equivalente y pongas la explicación de lo que está haciendo la consulta. Hint: usa la columna \"n\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resuelve el task 3 aqui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el tokenizador\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Definir las palabras clave relacionadas con sustancias ilícitas y tokenizarlas\n",
    "keywords = [\"weed\", \"alcohol\", \"drink\", \"smoke\", \"drug\"]\n",
    "tokenized_keywords = [tokenizer(keyword)[\"input_ids\"][0] for keyword in keywords]\n",
    "\n",
    "# Crear la consulta para buscar los textos que contienen alguna de las palabras clave tokenizadas\n",
    "query = \" OR \".join([f'vector LIKE \"%{token}%\"' for token in tokenized_keywords])\n",
    "\n",
    "# Ejecutar la consulta en la base de datos\n",
    "result_df = (\n",
    "    db[\"tabla\"]\n",
    "    .search()\n",
    "    .where(query)\n",
    "    .select([\"name\", \"vector\"])\n",
    "    .limit(11)\n",
    "    .to_pandas()\n",
    ")\n",
    "\n",
    "#lo que estoy haciendo es selecionar todos los textos que tengan algo que ver con\n",
    "#sustancias ilicitas para esto defini algunas keyword en las que creo que se centraria los mensajes\n",
    "#pero no puedo meterle las palabras simplemente ya que el db ya está tokenizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Query en SQL equivalente:\n",
    "- Explicacion: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4:\n",
    "Inventa un mensaje de texto que tu podrías escribirle a un amigo. Tokenizalo y ponlo en el formato adecuado para hacer un vector query. Quiero que me regreses el mensaje más parecido al mensaje que inventaste (OJO: quiero el texto, no el embedding). HINT: Hay que decodear el resultado del query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuevo_mensaje=\"lets got to a party and drink a lot \"\n",
    "nuevo_mensaje_embeded = tokenizer(nuevo_mensaje)[\"input_ids\"]\n",
    "\n",
    "n = 300\n",
    "def ajustar_vector(input, n):\n",
    "    output = input[:n]\n",
    "    \n",
    "    # Si la lista es más corta que el tamaño objetivo, rellenar con 0.0\n",
    "    while len(output) < n:\n",
    "        output.append(0)\n",
    "    \n",
    "    return output\n",
    "nuevo_mensaje_embeded = ajustar_vector(nuevo_mensaje_embeded, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resuelve el task 4 aqui\n",
    "result_df = (\n",
    "    db[\"tabla\"]\n",
    "    .search(nuevo_mensaje_embeded)\n",
    "    .metric(\"cosine\")\n",
    "    .select([\"name\", \"vector\"])\n",
    "    .limit(1)\n",
    "    .to_pandas()\n",
    ")\n",
    "\n",
    "mensaje_similar = result_df[\"name\"].iloc[0]\n",
    "\n",
    "# Obtener el texto original del mensaje más similar\n",
    "texto_mensaje_similar = tokenizer.decode(df[df[\"n\"] == mensaje_similar][\"vector\"].iloc[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
